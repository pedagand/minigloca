\documentclass[a4paper, 12pt]{article}

\usepackage[francais]{babel}
\usepackage[T1]{fontenc}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{mathpartir}
\usepackage{tikz}
\usepackage{listings}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{color}
\usepackage{mathtools}
\usepackage{caption}

\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}

\floatname{algorithm}{Algorithme}

\lstset{frame=tb,
	language=caml,
	columns=[c]fixed,
	basicstyle=\small\ttfamily,
	keywordstyle=\bfseries,
	upquote=true,
	commentstyle=,
	breaklines=true,
	showstringspaces=false,
	stringstyle=\color{green}
}

\title{Minigloca}
\author{Vladislas de Haldat}

\begin{document}

\include{doc-macros.tex}

\maketitle
\newpage
\tableofcontents
\newpage

\section{Un langage impératif simple}
Pour commencer, définissons une syntaxe abstraite minimale que nous utiliserons tout le long de cette étude.
Cette syntaxe se composera de trois blocs fondamentaux que sont les expressions arithmétiques, les expressions
booléennes ainsi que les déclarations. Nous n'inclurons pas pour le moment la définition de routines au sein de cette syntaxe.

\subsection{Expressions arithmétiques}
Les expressions arithmétiques sont définies sur l'ensemble des entiers relatifs. On se donne les opérateurs
de l'addition, de la soustraction ainsi que de la multiplication. À ces opérateurs l'on pourra appliquer des
entiers ainsi que des identifiants de variables.

% \begin{dtype}{Int}
%   \inlinekind{n}\with{n\in\mathbb{Z}}
% \end{dtype}
% \begin{dtype}{Id}
%   \inlinekind{x}
%   \kind{y}
%   \kind{z}
%   \kind{\ldots}
% \end{dtype}
% \begin{dtype}{Exp_a}
%   \inlinekind{Int}
%   \kind{Id}
%   \kind{op_A (a_1, a_2)}
%   \with{op_A \in\{+, -, \times\}}
% \end{dtype}

\begin{align*}
  Int   & ::= n                               & n \in \mathbb{Z}          \\
  Id    & ::= x \mid y \mid z \mid \cdots                                 \\
  Exp_a & ::= Int \mid Id \mid op_A(a_1, a_2) & op_A \in \{+, -, \times\}
\end{align*}
\captionof{figure}{Expression arithmétique}

\subsection{Expressions booléennes}
Les expressions booléennes nous permettent d'introduire la comparaison entre deux expressions arithmétiques,
ainsi que les opérateurs booléens sur les expressions booléennes.

\begin{dtype}{Exp_b}
  \inlinekind{\textbf{true}}
  \kind{\textbf{false}}\\
  \akind{op_R (a_1, a_2)}\with{op_R\in\{<, =\}}
  \akind{op_B (b_1, b_2)}\with{op_B\in\{\wedge, \vee\}}
  \akind{\neg{b}}
\end{dtype}
\captionof{figure}{Expression booléenne}

\subsection{Déclarations}
Les déclarations nous permettent de donner forme à notre langage en définissant la séquence de deux instructions ainsi
que les gardes booléennes, sur une condition ou une boucle. On pose la déclaration de la manière suivante,

\begin{dtype}{Stm}
  \inlinekind{\sassign{x}{a}}\\
  \akind{\sseq{s_1}{s_2}}\\
  \akind{\sskip}\\
  \akind{\sifthenelse{b}{s_1}{s_2}}\\
  \akind{\swhiledo{b}{s}}
\end{dtype}
Dans le reste de cette étude, on parlera aussi de déclaration en désignant un programme étant donné que les analyses statiques
n'ont pas besoin d'évaluer la déclaration pour se construire. On notera aussi que la routine peut pour le moment être
simplement vue comme la déclaration d'une boucle. 

\subsection{Exemple}
Voici un premier exemple sur ce langage
\begin{lstlisting}[tabsize=2]
a := 1;
b := 20;

if a = 3 then
	c := 4
else
	c := 6
endif;

while b < 100 do
	b := b + 1
done
\end{lstlisting}

\subsection{Arbre de syntaxe abstraite}
De manière à construire nos programmes à partir de ce qui a été précédemment énoncé, il est nécessaire d'introduire 
l'arbre de syntaxe abstraite. Ce modèle de représentation permet de parcourir la déclaration et d'en extraire n'importe
quelle fonctions (dans notre cas les différents opérateurs arithmétiques, booléens, ou d'affectation). 
Cela nous servira particulièrement lors des différentes analyses qui seront effectuées, et notamment l'analyse par flot de contrôle. 
Cette dernière ne requièrant pas la connaissance de l'ordre d'exécution de la déclaration, les AST sont tout-à-fait adaptés.
Voici un exemple d'AST sur notre langage.

\begin{center}\input{ast.fig}\end{center}
\captionof{figure}{Arbre de syntaxe abstraite}

\section{Interpréteur et sémantique}
Dans cette section, on s'atèle à décrire l'interpréteur ainsi que la sémantique sur notre petit langage impératif. On définit
$\mathbb{V}$ l'ensemble des variables utilisée dans un programme.
Dans ce cadre là, on définit l'état d'un programme (sur la pile) par une bijection entre les identifiants de variables et leurs valeurs, 
dans notre cas, des entiers relatifs,
\[\sigma : \mathbb{V} \longrightarrow \mathbb{Z}\]
On introduire aussi la bijection $\mu_B$,
\[\mu_B : \mathcal{B} \longrightarrow \mathbb{B}\]
où $\mathcal{B} = \{\textbf{true}$, \textbf{false}\} les valeurs booléennes de notre langage et $\mathbb{B} = \{\top, \bot\}$ 
l'ensemble des valeurs booléennes natives à OCaml. Pour la suite, on considèrera l'ensemble des états $\mathcal{S} = \mathbb{Z}^\mathbb{V}$.
Ces derniers décriront l'évolution de l'exécution du programme et nous permettront de formaliser un interpréteur.

\subsection{Interpréteur}
Pour le moment, il n'est pas nécessaire d'implémenter un compilateur pour notre langage, un interpréteur suffira.
Celui-ci servira notamment à valider les tests unitaires lorsque les premières analyses statiques seront appliquées
à nos programmes.
Sur les expressions arithmétiques, définies dans le section précédente, on pose l'application suivante,
\begin{align*}
	\intrfun{a}{A}:\mathcal{S}&\longrightarrow\mathbb{Z}\\
	\intrfun{x}{A}\sigma&\longmapsto\sigma(x)\\
	\intrfun{op_A(a_1, a_2)}{A}\sigma&\longmapsto\hat{op}_A(\intr{a_1}{A}{\sigma}\text{, }\intr{a_2}{A}{\sigma})\\
\end{align*}
De la même manière, on définit l'application qui suit sur les expressions booléennes,
\begin{align*}
	\intrfun{b}{B}:\mathcal{S}&\longrightarrow\mathbb{B}\\	
	\intrfun{\textbf{true}}{B}\sigma&\longmapsto\top\\
	\intrfun{\textbf{false}}{B}\sigma&\longmapsto\bot\\
	\intrfun{op_R(a_1, a_2)}{B}\sigma &\longmapsto \hat{op}_R(\intr{a_1}{A}{\sigma}\text{, }\intr{a_2}{A}{\sigma})\\
	\intrfun{op_B(b_1, b_2)}{B}\sigma&\longmapsto \hat{op}_B(\intr{b_1}{B}{\sigma}\text{, }\intr{b_2}{B}{\sigma})\\
	\intrfun{\neg b}{B}\sigma&\longmapsto\hat{\neg}\intr{b}{B}{\sigma}
\end{align*}
Les opérateurs de la forme $\hat{op}$ représentent les opérateurs natifs à OCaml.

\subsection{Sémantique}
Maintenant que nous avons correctement défini l'interpréteur, il est possible 
de construire la sémantique du langage. Cela permet également de développer 
un ensemble de règles logiques. La syntaxe de la déclaration des règles,
prenant en compte nos état et déclaration, sera donc de la forme suivante,

\[s, \Sigma \longrightarrow s', \Sigma'\]
\\
où $s$ est la première déclaration, $s'$ celle qui suit après son exécution, 
$\Sigma$ l'état initial et $\Sigma'$  l'état successeur. Commençons 
par la déclaration vide,

\srule{ }{\semanticd{\sskip}{\sigma}{\emptyset}{\sigma}}

Poursuivons avec la déclaration de l'affectation,
\srule{ }{\semanticd{\sassign{x}{a}}{\sigma}{\emptyset}{\sigma'[Id\longmapsto\intr{a}{A}{\sigma}]}}
Ici, le nouvel état $\sigma'$ lie l'identifiant de la variable assignée, à la 
valeur de l'expression arithmétique $\intrfun{a}{A}$. Poursuivons avec la règle 
de la séquence entre deux déclarations, d'une part si la première termine,
\srule{\semanticd{s_1}{\sigma}{\emptyset}{\sigma'}}
{\semanticd{\sseq{s_1}{s_2}}{\sigma}{s_2}{\sigma'}}
D'autre part, si la première ne termine pas,
\srule{\semanticd{s_1}{\sigma}{s_1'}{\sigma'}}
{\semanticd{\sseq{s_1}{s_2}}{\sigma}{\sseq{s_1'}{s_2}}{\sigma'}}
La condition peut se formaliser de la sorte dans le cas où la garde est vérifiée,
\srule{\intr{b}{B}{\sigma}}
{\semanticd{\sifthenelse{b}{s_1}{s_2}}{\sigma}{s_1}{\sigma}}
Dans le cas où elle ne l'est pas,
\srule{\neg\intr{b}{B}{\sigma}}
{\semanticd{\sifthenelse{b}{s_1}{s_2}}{\sigma}{s_2}{\sigma}}
On peut déclarer la règle qui suit pour la déclaration while,
\srule{\intr{b}{B}{\sigma}}
{\semanticd{\swhiledo{b}{s}}{\sigma}{\sseq{s}{\swhiledo{b}{s}}}{\sigma}}
\captionof{figure}{Sémantique}

\section{Prérequis à l'analyse}
De manière à pouvoir travailler avec les analyses de flot de données et de flot de contrôle, 
il est nécessaire d'approfondir le principe d'étiquettage, déjà abordé à la fin de la première section.

\subsection{Blocs}
De manière à faciliter l'analyse sur une déclaration, il est utile de partitionner et de factoriser notre représentation du code. 
Les blocs ont été évoqués dans la partie précédente comme des sous-parties de l'ensemble des déclarations atomiques dans un programme. 
Étant donné l'aspect rudimentaire de notre langage, on peut simplement définir le bloc de la sorte,

\begin{dtype}{Block}
	\inlinekind{\sassign{x}{a}}\\
	\akind{b}\\
	\akind{\sskip}
\end{dtype}

On remarquera que le bloc est défini de manière atomique. Cependant il est tout-à-fait possible de considérer des séquences
d'instructions comme un bloc à part entière, ce qui pourra, dans certains cas, largement simplifier la représentation du flot
de contrôle.

\subsection{Étiquettage}
On définit $\mathbb{L}\subseteq\mathbb{N}$ l'ensemble des étiquettes d'une déclaration. Introduisons l'application suivante,
\[
	\lambda: Block \longrightarrow \mathbb{L}
\]
\newline
qui prend un bloc et retourne une étiquette unique par rapport au reste des blocs d'une déclaration $s$.
\newline
\newline
\definition{Soient $s \in Stm$ et $(l_n)_{n\in\mathbb{N}}$ une liste d'étiquettes construite par $\lambda$ sur $s$.
On dit que $\lambda$ est bien formée si et seulement}
si $\forall i \in \mathbb{N}, \forall j \in \mathbb{N}$ tel que $i \neq j$ alors $l_i \neq l_j$.
\newline
\newline
À terme, on voudra représenter le programme par un graphe (le graphe de flot de contrôle) dont chaque noeud représentera un bloc. 
Étant donné une déclaration $s$, on a pour  $b$ un bloc de $s$ que $\delta_-(b) = 1$ le degré entrant et $\delta_+(b) \ge 1$ 
le degré sortant du noeud correspondant dans le graphe de flot de contrôle.
On peut maintenant définir la fonction $init$, qui retournera la première étiquette rencontrée dans une déclaration,

\begin{align*}
	init : Stm &\longrightarrow \mathbb{L}\\
	(\sassign{x}{a})^l &\longmapsto l\\
	\sseq{s_1}{s_2} &\longmapsto init(s_1)\\
	{\sskip}^l &\longmapsto l\\
	\sifthenelse{b^l}{s_1}{s_2} &\longmapsto l\\
	\swhiledo{b^l}{s} &\longmapsto l
\end{align*}
Comme expliqué précédemment, il est aussi nécessaire de déclarer une fonction $final$ qui retournera l'ensemble des étiquettes finales 
à la fin d'un bloc.

\begin{align*}
	final : Stm &\longrightarrow \mathcal{P}(\mathbb{L})\\
	(\sassign{x}{a})^l &\longmapsto \{l\}\\
	\sseq{s_1}{s_2} &\longmapsto final(s_2)\\
	\sskip^l &\longmapsto \{l\}\\
	\sifthenelse{b^l}{s_1}{s_2} &\longmapsto final(s_1) \cup final(s_2)\\
	\swhiledo{b^l}{s} &\longmapsto \{l\}
\end{align*}

\subsection{Flots}
\begin{definition}
	Un graphe de flot de contrôle (CFG) est un graphe orienté dont les noeuds représentent les blocs et les arcs un
	flot de contrôle (de même orientation que l'exécution du programme). Dans notre cas, le graphe de flot de
	contrôle aura un seul noeud d'entrée et peut admettre plusieurs noeuds de sortie.
\end{definition}
\\
En voici quelques exemples,
\begin{center}\input{cfg.fig}\end{center}
\captionof{figure}{Graphe de flot de contrôle}

De manière à correctement identifier chaque bloc lors d'une analyse statique, il nous faut les lier à une étiquette
unique. Pour ce faire, 
on pose l'application $blocks$ définie par,
\begin{align*}
	blocks : Stm &\longrightarrow \mathcal{P}(\mathbb{L} \times Block)\\
	(\sassign{x}{a})^l&\longmapsto\{(l, x := a)\}\\
	\sseq{s_1}{s_2} &\longmapsto blocks(s_1) \cup blocks(s_2)\\
	\sskip^l &\longmapsto \{(l, \textit{Skip})\}\\
	\sifthenelse{b^l}{s_1}{s_2} &\longmapsto \{(l, b)\}\cup blocks(s_1)\cup blocks(s_2)\\
	\swhiledo{b^l}{s} &\longmapsto \{(l, b)\}\cup blocks(s)
\end{align*}
À partir de là, il est possible de formaliser les graphes orientés de flot. Comme déjà expliqué, les blocs en représentent les noeuds, 
et le passage vers le bloc suivant est formulé par un arc.

Nous avons désormais toutes les structures nécéssaires à la construction de notre graphe de flot. Pour le moment, il s'agira de 
le construire naïvement, l'on reviendra plus tard sur les optimisations possibles. Ainsi, une manière simple de représenter ce 
graphe est de considérer l'ensemble des couples d'étiquettes qui indiqueront un arc d'un bloc à un autre. 
Considérons donc l'application suivante, 

\begin{align*}
	flow : Stm &\longrightarrow \mathcal{P}(\mathbb{L}^2) \\
	\sassign{x}{a} &\longmapsto \emptyset \\
	\sskip &\longmapsto \emptyset \\
	\sseq{s_1}{s_2} &\longmapsto flow(s_1) \cup flow(s_2) \cup [final(s_1)\times\{init(s_2)\}] \\
	\sifthenelse{b^l}{s_1}{s_2} &\longmapsto flow(s_1)\cup flow(s_2)\cup\{(l, init(s_1)),(l, init(s_2))\} \\
	\swhiledo{b^l}{s} &\longmapsto flow(s)\cup\{(l,init(s))\}\cup[final(s)\times\{l\}]
\end{align*}
On introduira aussi un accès à l'ensemble des successeurs d'un bloc par l'application,
\begin{align*}
	succ : \mathbb{L} &\longrightarrow \mathcal{P}(\mathbb{L})\\
	l &\longmapsto \{l' \in \mathbb{L} \mid (l, l') \in \mathcal{G}_V\},
\end{align*}
où $\mathcal{G}_V$ est l'ensemble des arcs du graphe de flot de contrôle du programme. Réciproquement, on 
utilisera $pred$ qui donnera cette fois-ci accès aux prédecesseurs d'un bloc.

\section{Analyse de vivacité}
La première analyse statique sur laquelle nous travaillons est l'analyse de vivacité des variables dans nos programmes. 
Il s'agira donc de déterminer pour chaque bloc, l'ensemble de variables encore vivantes, c'est-à-dire
encore utilisées une fois le présent bloc passé. Pour cela, il faut au préalable déclarer quelques ensembles 
nécessaires à cette analyse.

\subsection{Description ensembliste d'un programme}
Soit $\mathbb{V}$ l'ensemble des variables assignées dans une déclaration. On se donne alors $(\mathcal{P}(\mathbb{V}), \subseteq)$
l'ensemble partiellement ordonné des parties de $\mathbb{V}$, le treillis sur lequel on travaillera désormais.
\\
Pour $l \in \mathbb{L}$ l'étiquette d'un bloc, on définit les ensembles qui suivent,
\[\sgen{l} \subseteq \mathcal{P}(\mathbb{V})\]
l'ensemble des variables appelées dans le bloc en question et,
\[\skill{l} \subseteq \mathcal{P}(\mathbb{V})\]
l'ensemble des variables nouvellement assignées donc considérées pour lors comme mortes. Enfin on se donnera,
\[vars_a : a \longrightarrow \mathcal{P}(\mathbb{V})\]
l'ensemble des identifiants de variables présentes dans une expression arithmétique et,
\[vars_b : b \longrightarrow \mathcal{P}(\mathbb{V})\]
l'ensemble des identifiants de variables présentes dans une expression booléenne.
\\
On définit maintenant la construction de ces deux ensembles à partir d'un bloc comme,
\begin{align*}
	gen : Block &\longrightarrow \mathcal{P}(\mathbb{V})\\
	\sassign{x}{a} &\longmapsto vars_a(a)\\
	\sskip &\longmapsto \emptyset\\
	b &\longmapsto vars_b(b)
\end{align*}
génère l'ensemble $\sgen{l}$ et,
\begin{align*}
	kill : Block &\longrightarrow \mathcal{P}(\mathbb{V})\\
	\sassign{x}{a} &\longmapsto \{x\}\\
	\sskip &\longmapsto \emptyset\\
	b &\longmapsto \emptyset
\end{align*}
génère l'ensemble $\skill{l}$.
\\
\\
Notre langage étant un langage impératif, il est par conséquent naturel d'effectuer cette analyse de bas en haut dans le parcours des déclarations,
de manière à déterminer cette vivacité. Pour pouvoir donner forme à cette analyse, on déclare deux ensembles de variables vivantes à l'entrée, et à la
sortie d'un bloc. Ils nous permettront par la suite de résoudre un système d'équation par itération et d'y trouver un point fixe. 
On les définira comme tels,

\[
	LIVE_{out}[l] = 
	\begin{dcases*}
		\emptyset &si $succ(l) = \emptyset $\,, \\
		\bigcup\limits_{p\in succ(s)} LIVE_{in}[p] &sinon\,,
	\end{dcases*}
\]
l'ensemble des variables vivantes à la sortie d'un bloc et,

\[
	LIVE_{in}[l] = \sgen{l} \cup (LIVE_{out}[l] - \skill{l}),
\]
l'ensemble des variables vivantes à l'entrée d'un bloc.
\\
\\
\begin{notation}
	Si $Z$ est une solution du système d'équation sur un treillis $(\mathcal{P}(L), \sqsubseteq)$, on notera aussi $Z_l \subseteq \mathcal{P}(L)$
	la solution au bloc d'étiquette $l$.
\end{notation}
\\
À ce stade, nous ne pouvons pas encore effectuer l'analyse de vivacité sur nos programmes, il nous manque en
effet un algorithme d'itération qui puisse résoudre ce système d'équation et déterminer le point fixe minimal.

\subsection{Monotonie}
La monotonie des ensembles de flot de données nous permet de déterminer l'existence d'un point fixe sur notre
fonction. Cela sera donc utile pour fournir un algorithme qui termine, lors de la construction de ces ensembles, 
étant donné que l'ensemble des variables d'une déclaration est supposé fini.
\\
\\
\begin{lemma}
	Pour tout $l \in \mathbb{L}$, $\livein{l}$ et $\liveout{l}$ sont monotones.
\end{lemma}
\\
\begin{proof}	
Posons l'application pour $l \in \mathbb{L}$,
\begin{align*}
	f_l : \mathcal{P}(\mathbb{V}) &\longrightarrow \mathcal{P}(\mathbb{V}) \\
	\mathcal{O} &\longmapsto \sgen{l} \cup (\mathcal{O} - \skill{l})
\end{align*}
Soient $K, K' \in \mathcal{P}(\mathbb{V})$ tels que $K \subseteq K'$, alors on a,
\begin{align*}
	K - \skill{l} &\subseteq K' - \skill{l} \text{ et,}\\
	\sgen{l} \cup (K - \skill{l}) &\subseteq \sgen{l} \cup (K' - \skill{l}).
\end{align*}
Donc $f_l(K) \subseteq f_l(K')$ ce qui implique $f_l$ monotone. Comme $\liveout{l}$ est une union de tous les $\livein{p}$
de ses successeurs $p$ et que $f_l$ est une fonction monotone, on a que $\liveout{l}$ est monotone pour tout $l$, par monotonie 
de l'union. Il vient de plus que $\livein{l}$, composé avec $\liveout{l}$, est monotone.
Ainsi $\livein{l}$ et $\liveout{l}$ sont monotones et produisent des ensembles finis, étant donné
la finitude de $\mathbb{V}$.
\end{proof}

\subsection{Point fixe}
Revenons-en à l'existence d'un point fixe, pour pouvoir construire un algorithme itératif. En effet,
on a que $(\mathcal{P}(\mathbb{V}), \sqsubseteq)$ est un ensemble muni d'un ordre partiel et est fini.
Ainsi, grâce à la monotonie de nos deux ensembles $\livein{\cdot}$ et $\liveout{\cdot}$ démontrée ci-dessus, il
vient qu'à chaque itération de notre algorithme, l'ensemble produit sera soit identique à l'ensemble précédent,
soit plus gros et la finitude de l'ensemble des variables assure qu'il ne pourra par grossir indéfiniment. 
Il faut cependant aussi avoir la garantie que la solution trouvée soit la plus petite qui puisse exister.
On se sert pour y parvenir du théorème du point fixe de Kleene.
\\
\\
\begin{theorem}	
	Soit $(L, \sqsubseteq)$ un ordre partiellement ordonné, avec un plus petit élément  $\perp$ et soit
	une application $f : L \longrightarrow L$ monotone. Alors il existe un point fixe minimal qui est le suprémum de la suite,
	\[\perp \sqsubseteq f(\perp) \sqsubseteq f^2(\perp) \sqsubseteq \cdots \sqsubseteq f^k(\perp) \sqsubseteq \cdots\]
\end{theorem}

Commençons par établir un algorithme de point fixe naïf. Il s'agira simplement de recalculer l'entierté du système
d'équation à chaque itération, jusqu'à trouver le point fixe minimal (par le théorème de Kleene).

\begin{algorithm}[H]
	\caption{Itération du point fixe}
	\begin{algorithmic}
		\State {Soit $s$ la déclaration}
		\State $\mathcal{B} \leftarrow blocks(s)$
		\State {Soit $L$ l'ensemble des étiquettes de $\mathcal{B}$}
		\For{$l \in L$}
		\State $live_{in}[l] \leftarrow \emptyset$
		\State $live_{out}[l] \leftarrow \emptyset$
		\EndFor
		\While{$live_{in} \ne live_{in}' \textbf{ ou } live_{out} \ne live_{out}'$}
		\For{$l \in L$}
		\State $live_{out}[l] \leftarrow \bigcup\limits_{p\in succ(l)} live_{in}[p]$
		\State $live_{in}[l] \leftarrow \sgen[l] \cup (live_{out}[l] - \skill{l})$
		\EndFor
		\EndWhile
	\end{algorithmic}
\end{algorithm}
Si $n = \#\mathcal{B}$, alors cet algorithme a une complexité en $O(kn)$, où $k$ est la hauteur du diagramme
de Hasse sur $(\mathcal{P}(\mathbb{V}), \subseteq)$. Cela se montre grâce à la monotonie de $\livein{\cdot}$, 
celle-ci ne faisant que croître vers $\top$.
Concernant l'implémentation, il peut être intéressant de définir $live$ comme une structure binaire de taille au
moins $m$ bits avec $m = \#\mathbb{V}$.
En outre, une amélioration peut être simplement faite sur l'algorithme décrit ci-dessus. On peut remarquer qu'à chaque 
modification effective de $\liveout{\cdot}$, seuls les blocs prédecesseurs seront susceptibles de s'altérer. Cela
vient du fait que l'analyse se fait de bas en haut. Donc
au lieu d'itérer à nouveau sur l'ensemble des blocs, il suffit d'itérer uniquement sur les blocs prédecesseurs au dernier
bloc altéré. L'on décrira ce second algorithme plus bas dans la section.
\\
\\
\begin{lemma}
	Étant donné une déclaration $s$, à partir de $\bot$ l'algorithme d'itération du point fixe trouve effectivement
	un point fixe sur $s$ et termine.
\end{lemma}
\\
\begin{proof}
La correction de l'algorithme repose en partie sur ce qui a été dit en amont de cette section.
En effet, à chaque itération dans laquelle on vérifie si le point fixe a ou non été atteint, on met à jour les
tableaux $live_{in}[l]$ et $live_{out}[l]$ pour tout bloc d'étiquette $l$. Mais comme pour ce faire, on utilise deux fonctions monotones croissantes et qu'on suppose
l'ensemble de nos variables fini, on est assuré de trouver le plus petit point fixe existant (par le théorème de Kleene) et ainsi de terminer l'algorithme.
\end{proof}
\\
Dans cette étude, nous travaillerons avec les deux algorithmes évoqués pour réaliser les tests unitaires. Ci-dessous, un aperçu du fonctionnement
de l'algorithme par worklist et sa correction.

\begin{algorithm}[H]
	\caption{Itération du point fixe (worklist)}
	\begin{algorithmic}
		\State {Soit $s$ la déclaration}
		\State $\mathcal{B} \leftarrow blocks(s)$
		\State {Soit $L$ l'ensemble des étiquettes de $\mathcal{B}$}
		\State $\mathcal{Q} \leftarrow L$ une queue des étiquettes
		\For{$l \in L$}
		\State $live_{in}[l] \leftarrow \emptyset$
		\State $live_{out}[l] \leftarrow \emptyset$
		\EndFor
		\While{$\#\mathcal{Q} > 0$}
		\State $q \leftarrow \mathcal{Q}$.pop
		\State $live_{in}'[q] \leftarrow live_{in}[q]$
		\State $live_{out}[q] \leftarrow \bigcup\limits_{p\in succ(q)} live_{in}[p]$
		\State $live_{in}[q] \leftarrow \sgen{q} \cup (live_{out}[q] - \skill{q})$
		\If {$live_{in}[q] \ne live_{in}'[q]$}
		\State $\mathcal{Q}$.push(pred(q))
		\EndIf
		\EndWhile
	\end{algorithmic}
\end{algorithm}
\noindent
\begin{lemma}
	Étant donné une déclaration $s$, à partir de $\bot$ l'algorithme par worklist trouve effectivement
	un point fixe sur $s$ et termine.
\end{lemma}
\\
\begin{proof}
On remarque qu'à la modification de l'analyse de vivacité $\liveout{\cdot}$ à la sortie d'un bloc,
les blocs successeurs ne seront nullement impactés. En effet seuls les blocs prédecesseurs seront
altérés d'après notre définition ensembliste de la vivacité. L'algorithme est donc une restriction de l'algorithme naïf vu plus
haut. À chaque modification effective, on ajoute à la queue des blocs à traiter en l'occurrence, l'ensemble des blocs
prédecesseurs au bloc en question. Ensuite, par monotonie de $\livein{\cdot}$ et $\liveout{\cdot}$ et
par finitude de l'ensemble de nos variables, on est assuré de trouver le plus petit point fixe existant
et ainsi terminer l'algorithme.
\end{proof}
\subsection{Exemple}
\noindent
Considérons une déclaration simple comme celle-ci,
\begin{lstlisting}[tabsize=2]
	a := 0;
	b := a;
	while a < 100 do
		a := a + 1
	done;
	c := b + a
\end{lstlisting}
On obtient donc l'analyse de vivacité suivante, en appliquant l'un des deux algorithmes explicités ci-dessus,
\\
\begin{center}
	\begin{tabular}{||c|l|r|l||}
	\hline
	Étiq. & Bloc & $\livein{\cdot}$ & $\liveout{\cdot}$ \\
	\hline
	1 & a := 0 & $\emptyset$ & $\{a\}$\\
	2 & b := a & $\{a\}$ & $\{a, b\}$\\
	3 & while a < 100 do & $\{a, b\}$ & $\{a, b\}$\\
	4 & a := a + 1 & $\{a, b\}$ & $\{a, b\}$\\
	- & done & - & -\\
	5 & c := b + a & $\{a, b\}$ & $\emptyset$\\
	\hline
	\end{tabular}
\end{center}
\section{Élimination de code mort}
Cette analyse permet une première optimisation qu'est l'élimination de code mort. Elle consiste, en pré-compilation,
à générer un nouveau code à partir du code initial, dans lequel le code dit mort n'est plus présent.
On définira ici le code mort comme un ensemble de blocs d'une déclaration qui ne seront jamais utilisés.

\subsection{Réduction naïve}
Cette optimisation peut en premier lieu être abordée de manière naïve. On notera $\mathcal{A}$ l'analyse de flot de donnée.
Considérons $\Delta$ la fonction d'élimination du code mort définie comme:
\[\Delta : \mathcal{A} \times Stm \longrightarrow Stm\]
qui, à partir d'une analyse de flot de donnée et d'une déclaration, produit un nouveau code. $\Delta$ est itérée
jusqu'à atteindre un point fixe sur la déclaration initiale, et la déclaration finale. À chaque itération,
l'analyse de vivacité de la nouvelle déclaration est à nouveau calculée à partir de $\bot$.

\subsection{Exemple}
\noindent
Considérons la déclaration suivante.
\begin{lstlisting}[tabsize=2]
	a := 1;
	b := 0;
	c := a + b;
	while b < 100 do
		b := b + 1
	done;
	d := 2 * c
\end{lstlisting}
On a donc l'analyse de vivacité,
\begin{center}
\begin{tabular}{||c|l|r|l||}
\hline
Étiq. & Bloc & $\livein{\cdot}$ & $\liveout{\cdot}$ \\
\hline
1 & a := 1 & $\emptyset$ & $\{a\}$\\
2 & b := 20 & $\{a\}$ & $\{a, b\}$\\
3 & c := a + b & $\{a, b\}$ & $\{b, c\}$\\
4 & while b < 100 do & $\{b, c\}$ & $\{b, c\}$\\
5 & b := b + 1 & $\{b, c\}$ & $\{b, c\}$\\
- & done & - & -\\
6 & d := 2 * c & $\{c\}$ & $\emptyset$\\
\hline
\end{tabular}
\end{center}
On remarque que $\liveout{6} = \emptyset$ donc $d$ est morte après avoir été affectée. L'algorithme applique donc sur ce bloc
une tranformation vers une instruction $Skip$, puis calcule à nouveau l'analyse, à partir de $\bot$. On obtient alors l'analyse
de vivacité,
\begin{center}
	\begin{tabular}{||c|l|r|l||}
	\hline
	Étiq. & Bloc & $\livein{\cdot}$ & $\liveout{\cdot}$ \\
	\hline
	1 & a := 1 & $\emptyset$ & $\{a\}$\\
	2 & b := 20 & $\{a\}$ & $\{a, b\}$\\
	3 & c := a + b & $\{a, b\}$ & $\{b\}$\\
	4 & while b < 100 do & $\{b\}$ & $\{b\}$\\
	5 & b := b + 1 & $\{b\}$ & $\{b\}$\\
	- & done & - & -\\
	6 & () & $\emptyset$ & $\emptyset$\\
	\hline
	\end{tabular}
\end{center}
De la même manière, $\liveout{3} \cap \{c\} = \emptyset$ donc cette variable est considérée comme morte dans la suite de la déclaration.
L'algorithme transforme donc le bloc en question en $Skip$ puis calcule, de la même manière, l'analyse de vivacité.
\begin{center}
	\begin{tabular}{||c|l|r|l||}
	\hline
	Étiq. & Bloc & $\livein{\cdot}$ & $\liveout{\cdot}$ \\
	\hline
	1 & a := 1 & $\emptyset$ & $\emptyset$\\
	2 & b := 20 & $\emptyset$ & $\{b\}$\\
	3 & () & $\{b\}$ & $\{b\}$\\
	4 & while b < 100 do & $\{b\}$ & $\{b\}$\\
	5 & b := b + 1 & $\{b\}$ & $\{b\}$\\
	- & done & - & -\\
	6 & () & $\emptyset$ & $\emptyset$\\
	\hline
	\end{tabular}
\end{center}
Enfin, $\liveout{1} = \emptyset$ donc la variable $a$ est morte dans la suite de la déclaration, le bloc d'étiquette 1 est à son tour
réduit, et l'on atteint un point fixe sur la réduction, c'est-à-dire que l'algorithme ne peut plus réduire quoi que ce soit et renvoie
toujours la même déclaration. Cela termine avec la déclaration réduite,
\begin{lstlisting}[tabsize=2]
	();
	b := 0;
	();
	while b < 100 do
		b := b + 1
	done;
	()
\end{lstlisting}
\subsection{Incrémentalisation de la réduction}
Cette réduction naïve est cependant particulièrement inefficace. En effet, elle doit à chaque itération calculer
l'analyse à partir de $\bot$ et cela peut s'avérer lourd lorsque nos programmes se composent de milliers de blocs.
On introduit donc la réduction de code mort par incrémentalisation, dans laquelle on essaye plutôt de calculer
la nouvelle analyse de vivacité à partir de la précédente, ce qui réduit considérablement le nombre d'opérations.
Pour résumer l'idée, voilà comment nous pourrions définir cette fois notre application de réduction,
\[\Delta : \mathcal{A} \times Stm \longrightarrow \mathcal{A} \times Stm \]
qui se rappelle récursivement, en utilisant la précédente analyse pour réduire la nouvelle déclaration.
\\
\\
\begin{notation}
	Par la suite, si $s \in Stm$ est une déclaration, on notera $\reduced{s}{\mathcal{L}}{Skip}$ cette même déclaration, 
	réduit aux blocs d'étiquette dans $\mathcal{L}$.
\end{notation}

Cependant, si on considère $s \in Stm$, $\mu_s$ son point fixe minimal et $s' = \reduced{s}{\mathcal{L}}{Skip}$ cette même déclaration
réduite, alors il n'est pas possible d'utiliser l'analyse de vivacité de $s$ pour poursuivre la réduction, étant donné qu'on ne peut pas garantir que
$\mu_{s} \sqsubseteq \mu_{s'}$ et que, comme démontré plus haut, la recherche de point fixe est croissante monotone uniquement. 
Il s'agit donc de trouver un moyen de suffisament décroître le précédent treillis.
Pour ce faire, on peut introduire un ensemble $\filterset$ qui agit comme un filtre de manière à supprimer l'information en trop. 
Pour convenablement filtrer le treillis de $s$, il faut ajouter de l'information à cette dernière. 
En effet, il est nécessaire qu'à chaque bloc $b$, il soit possible de connaître quel bloc sous-jacent
a besoin des variables vivantes en $b$. Le treillis utilisé manque alors d'information et n'est plus adapté, étant donné qu'il n'est pas en mesure
d'indiquer d'où vient la propagation d'une variable vivante. On essaye donc plutôt d'utiliser le treillis $(\mathcal{P}(\mathbb{L} \times \mathbb{V}), \sqsubseteq)$. 
\\
\\
En ce qui concerne la comparaison de ces paires, elle dépend de la manière avec laquelle l'algorithme de réduction parcourira la déclaration donnée.
En effet, si celui-ci se fait strictement de bas en haut, alors la comparaison peut uniquement se faire sur la variable. Nous préférerons néanmoins
que ce treillis serve quelque soit l'ordre dans lequel l'algorithme effectue la réduction, la comparaison se fera donc sur l'étiquette
et sur la variable. Une fois cela acquis, il est possible d'énoncer le théorème qui suit.
\\
\\
\begin{theorem}
	Soient $s \in Stm$, et $s' = \reduced{s}{\mathcal{L}}{Skip}$ une réduction sur
	l'ensemble d'étiquettes $\mathcal{L}$. Soient $\mu_s$ et $\mu_{s'}$ les point fixes minimaux respectifs
	des deux déclarations.
	Alors $\exists \filterset$ tel que $\forall l \in \mathbb{L}$,
	\[
		\mu_s[l] - \filterset \sqsubseteq \mu_{s'}[l]
	\]
	est un pré-point fixe de $\mu_{s'}[l]$.
\end{theorem}
\\
\\
\begin{proof}
	Soient $s \in Stm$ et $s' := s[\mathcal{L} \longmapsto Skip]$ la déclaration à partir de $s$ dont les blocs d'étiquette 
	dans $\mathcal{L}$ sont réduits. 
	On se place en outre sur le treillis $(\mathcal{P}(\mathbb{L} \times \mathbb{V}), \sqsubseteq)$. 
	Une fois cela donné, on redéfinit ce que sont $vars$, $gen$ et $kill$ de la manière qui suit, 
	\[vars_a : \mathbb{L} \times a \longrightarrow \mathcal{P}(\mathbb{L} \times \mathbb{V})\]
	retourne désormais l'ensemble des variables d'une expression arithmétique, liées chacunes à l'étiquette du bloc qui les génère et,
	\[vars_b : \mathbb{L} \times b \longrightarrow \mathcal{P}(\mathbb{L} \times \mathbb{V})\]
	retourne désormais l'ensemble des variables d'une expression booléennes, liées chacunes à l'étiquette du bloc qui les génère.
	\begin{align*}
		gen : \mathbb{L} \times Block &\longrightarrow \mathcal{P}(\mathbb{L} \times \mathbb{V})\\
		(l, \sassign{x}{a}) &\longmapsto vars(l, a)\\
		(l, \sskip) &\longmapsto \emptyset\\
		(l, b) &\longmapsto vars(l, b)
	\end{align*}
	génère maintenant l'ensemble $\sgen{l}$ et,
	\begin{align*}
		kill : Block &\longrightarrow \mathcal{P}(\mathbb{L} \times \mathbb{V})\\
		\sassign{x}{a} &\longmapsto \mathbb{L} \times \{a\}\\
		\sskip &\longmapsto \emptyset\\
		b &\longmapsto \emptyset
	\end{align*}
	génère maintenant l'ensemble $\skill{l}$.
	\\ 
	Par commodité pour la suite, on se donnera les applications,
	\begin{align*}
		g : \mathcal{P}(\mathbb{L} \times \mathbb{V}) &\longrightarrow \mathcal{P}(\mathbb{L} \times \mathbb{V})\\
		X_l &\longmapsto \bigcup\limits_{p\in succ(l)} \livein{p}
	\end{align*}
	monotone croissante, d'après la démonstration sur la monotonie de $\liveout{\cdot}$ ci-dessus et,
	\begin{align*}
		f : \mathcal{P}(\mathbb{L} \times \mathbb{V}) &\longrightarrow \mathcal{P}(\mathbb{L} \times \mathbb{V})\\
		X_l &\longmapsto \sgen{l} \cup (g(X_l) - \skill{l})
	\end{align*}
	également monotone croissante par la démonstration sur la monotonie de $\livein{\cdot}$ ci-dessus.
	On pose $E_s$ et $E_{s'}$ les deux systèmes d'équations respectivement de $s$ et de $s'$,
	\[
	E_s :
	\begin{dcases*}
		\livein{l} = f(\livein{l}) &$\forall l \in \mathbb{L} - \mathcal{L}$\,, \\
		\livein{k} = f(\livein{k}) &$\forall k \in \mathcal{L}$\,,
	\end{dcases*}
	\]
	et
	\[
	E_{s'} :
	\begin{dcases*}
		\livein{l} = f(\livein{l}) &$\forall l \in \mathbb{L} - \mathcal{L}$\,,\\
		\livein{k} = g(\livein{k}) &$\forall k \in \mathcal{L}$\,.
	\end{dcases*}
	\]
	Posons enfin, l'ensemble filtre tel que,
	\[
		\filterset = \{(l, v) \mid l \in \mathcal{L}, v \in \mathbb{V}\}
	\]
	On considère $Z = (Z_l)_{l\in \mathbb{L}}$ le plus petit point fixe de la déclaration $s$ \textit{i.e.} la plus petite solution du système $E_s$.
	Montrons d'abord que $\forall X = (X_l)_{l\in \mathbb{L}}$ solution de $E_{s'}$ alors,
	\[Z_k - \filterset \sqsubseteq X_k, \forall k \in \mathcal{L}\]
	D'après la définition de $gen$ sur ce treillis il vient que $\sgen{k} \sqsubseteq I_{\mathcal{L}}$, donc on a
	\[Z_k - \filterset = g(Z_k) - (\skill{k} - \{(k, v) \mid v \in \mathbb{V}\}) \sqsubseteq g(Z_k) \sqsubseteq g(X_k) = X_k\]
	comme on suppose $Z$ la plus petite solution du système $E_s$.
	Montrons ensuite que $\forall X = (X_l)_{l\in \mathbb{L}}$ solution de $E_{s'}$ on a,
	\[
		Z_l - \filterset \sqsubseteq X_l, \forall l \in \mathbb{L} - \mathcal{L}
	\]
	Dans ce cas, on remarque que 
	\[Z_l - X_l = Z_l \cap \filterset\] 
	mais aussi que, 
	\[X_l - Z_l \sqsubseteq \mathcal{U} = \bigcup\limits_{k\in \mathcal{L}} g(X_k) \cap \skill{k}\]
	et ainsi que,
	\[g(Z_l) - I_{\mathcal{L}} \sqsubseteq g(X_l)\]
	Donc on obtient que,
	\[
		Z_l - \filterset = \sgen{l} \cup (g(X_l) - \mathcal{U} - \skill{l}) \sqsubseteq f(X_l) = X_l
	\]
	Étant donné que $[g(X_l) - \mathcal{U}] \cap \filterset = \emptyset$ il n'est pas nécessaire de retirer à $\skill{l}$ les possibles
	paires d'étiquettes dans $\mathcal{L}$.
	Donc pour tout $l \in \mathbb{L}$ et pour tout $X$ solution du système $E_{s'}$, on a bien montré que $Z_l - \filterset \sqsubseteq X_l$ .
	Donc $Z$ est bien un pré-point fixe de $\mu_{s'}$, la plus petite solution de $E_{s'}$.
\end{proof}
\subsection{Exemple}
Illustrons tout ce qui vient d'être dit, avec une déclaration sur laquelle on applique une incrémentalisation, d'abord à l'aide du premier
treillis, puis à l'aide du second. Considérons la déclaration,
\begin{lstlisting}[tabsize=2]
	a := 0;
	b := a;
	while a < 100 do
		a := a + 1;
		c := a + b
	done;
	d := a * b * c
\end{lstlisting}
On trouve donc la première analyse de vivacité, à partir de $\bot$ sur le treillis $(\mathcal{P}(\mathbb{V}), \sqsubseteq)$
\begin{center}
	\begin{tabular}{||c|l|r|l||}
	\hline
	Étiq. & Bloc & $\livein{\cdot}$ & $\liveout{\cdot}$ \\
	\hline
	1 & a := 1 & $\emptyset$ & $\{a\}$\\
	2 & b := a & $\{a\}$ & $\{a, b\}$\\
	3 & while a < 100 do & $\{a, b\}$ & $\{a, b\}$\\
	4 & a := a + 1 & $\{a, b\}$ & $\{a, b\}$\\
	5 & c := a + b & $\{a, b\}$ & $\{a, b, c\}$\\
	- & done & - & -\\
	6 & d := a * b * c & $\{a, b, c\}$ & $\emptyset$\\
	\hline
	\end{tabular}
\end{center}
Supposons désormais, qu'on veuille réduire le bloc d'étiquette 5, et produire la nouvelle analyse de vivacité
à partir de celle décrite ci-dessus. En ce bloc, on a $\sgen{5} = \{a, b\}$ donc il faut propager la perte de vivacité
de ce bloc, vers ses blocs prédecesseurs. Vient le problème de l'origine de la vivacité sur une variable. En effet,
en l'état de ce treillis, il est impossible de savoir, ni qui utilise ces variables dans les blocs successeurs ni qui
maintient la vivacité, pour cause du bloc 5, dans les blocs prédecesseurs. Donc on ne peut pas garantir l'obtention d'un
pré-point fixe sur le code réduit. Essayons désormais le second treillis, 
$(\mathcal{P}(\mathbb{L} \times \mathbb{V}), \sqsubseteq)$. Cette fois-ci, on trouve la première analyse de vivacité, à partir
de $\bot$,
\begin{center}
	\begin{tabular}{||c|l|r|l||}
	\hline
	Étiq. & Bloc & $\livein{\cdot}$ & $\liveout{\cdot}$ \\
	\hline
	1 & a := 1 & $\emptyset$ & $\{a_2, a_3, a_4\}$\\
	2 & b := a & $\{a_2, a_3, a_4\}$ & $\{a_3, a_4, b_5, b_6\}$\\
	3 & while a < 100 do & $\{a_3, a_4, b_5, b_6\}$ & $\{a_4, b_5, b_6\}$\\
	4 & a := a + 1 & $\{a_4, b_5, b_6\}$ & $\{a_3, a_4, a_5, a_6, b_5, b_6\}$\\
	5 & c := a + b & $\{a_3, a_4, a_5, a_6, b_5, b_6\}$ & $\{a_3, a_4, a_6, b_5, b_6, c_6\}$\\
	- & done & - & -\\
	6 & d := a * b * c & $\{a_6, b_6, c_6\}$ & $\emptyset$\\
	\hline
	\end{tabular}
\end{center}
Réduisons désormais le bloc d'étiquette 5 et appliquons le filtre $I_{\{5\}}[\mu_P]$ sur l'analyse,
\begin{center}
	\begin{tabular}{||c|l|r|l||}
	\hline
	Étiq. & Bloc & $\livein{\cdot}$ & $\liveout{\cdot}$ \\
	\hline
	1 & a := 1 & $\emptyset$ & $\{a_2, a_3, a_4\}$\\
	2 & b := a & $\{a_2, a_3, a_4\}$ & $\{a_3, a_4, b_6\}$\\
	3 & while a < 100 do & $\{a_3, a_4, b_6\}$ & $\{a_4, b_6\}$\\
	4 & a := a + 1 & $\{a_4, b_6\}$ & $\{a_3, a_4, a_6, b_6\}$\\
	5 & () & $\{a_3, a_4, a_6, b_6\}$ & $\{a_3, a_4, a_6, b_6, c_6\}$\\
	- & done & - & -\\
	6 & d := a * b * c & $\{a_6, b_6, c_6\}$ & $\emptyset$\\
	\hline
	\end{tabular}
\end{center}
On a bien obtenu un pré-point fixe de cette déclaration réduite à partir de l'analyse de vivacité précédente.
Par la monotonie de l'itération, on peut donc faire croître le treillis et trouver le point fixe minimal. 
\section{Généralisation}
Ceci achève une première analyse statique qu'est l'analyse de vivacité. On s'essaye maintenant à déterminer
un lien plus général entre le prédicat, le treillis, et la transformation de code.
\subsection{Motivations}
On cherche à implémenter un meta-langage, qui servira entre notre langage et son compilateur, à orienter
ce dernier sur les optimisations à fournir. Pour ce faire, on se sert de prédicats qui valideront ou non la
transformation du code. Ces prédicats sont des applications prenant des éléments évaluables à la compilation, 
dans notre cas, des variables et plus tard, des routines.
Soit $P : \mathbb{V} \longrightarrow \{0, 1\}$ un prédicat quelconque, voici un exemple d'un tel programme,
\begin{lstlisting}[tabsize=2]
	...
	x := a;
	...
	y := 0;
	#if P(x)
		while(y < x) do
			y := y + 1
		done;
	#else
		y := 1;
	#endif
	...
\end{lstlisting}
La bonne utilisation de ces prédicats doit permettre la production d'un code assembleur minimal et le plus
optimal possible, ce qui s'avère essentiel lorsqu'on recherche de hautes performances d'exécution.
Revenons-en à un langage impératif tel que C pour illustrer cela. Supposons qu'on ait une simple fonction de division 
par 32 d'un entier sur 32 bits.
\begin{lstlisting}[tabsize=2, language=c]
	int div(int x)
	{
		return x/32;
	}
\end{lstlisting}
Le compilateur GCC produit alors le code assembleur suivant,
\begin{lstlisting}[tabsize=2]
	div:
		mov     eax, DWORD PTR [rbp-4]
		lea     edx, [rax+31]
		test    eax, eax
		cmovs   eax, edx
		sar     eax, 5
		ret
\end{lstlisting}
On suppose désormais que cette fonction sera appelée dans une boucle à ne traiter que des entiers signés sur 32 bits, positifs.
Alors ce code assembleur devient particulièrement inefficace étant donné qu'il considère le cas d'entiers
négatifs. En effet, on aimerait plutôt avoir quelque chose comme,
\begin{lstlisting}[tabsize=2]
	div:
		mov     eax, DWORD PTR [rbp-4]
		shr     eax, 5
		ret
\end{lstlisting}
ce qui est déjà bien mieux! On pourrait donc dans ce cas poser notre meta-langage sur l'appel de \textit{div}, avec un prédicat
défini par,
\[P : v \longmapsto \intrfun{v > 0 \vee v = 0}{B}\sigma\]
où $\sigma$ est l'état de nos variables sur la pile. Cela donnerait l'hypothèse d'un entier positif à la pré-compilation
et permettrait d'obtenir ce code assembleur spécifiquement.
\subsection{Première approche}
Entamons cette généralisation en partant d'équations similaires que sont, pour chaque bloc d'étiquette $l$,
\begin{align*}
	A_{in}[l] &= \sgen{l} \cup (A_{out}[l] - \skill{l})\\
	A_{out}[l]&= \bigcup\limits_{p \in \mathcal{K}_l} A_{in}[p]
\end{align*}
L'analyse de vivacité développée plus haut se faisait de bas en haut dans le parcours du CFG, on avait alors
\[\mathcal{K}_l = succ(l)\] 
Cependant, si l'analyse se fait de haut en bas, on aurait alors plutôt
\[\mathcal{K}_l = pred(l)\]
On peut énoncer une dimension supplémentaire sur l'analyse de sortie,
\[
	A_{out}[l] = \bigcap\limits_{p \in \mathcal{K}_l} A_{in}[p]	
\]

\section{Annexe}
\subsection{Validation}
\subsection{Tests unitaires}
\subsection{Générateur}
Dans l'optique de vérification de la robustesse de nos différentes analyses, il peut être bon de réaliser
des tests unitaires sur chacune d'elles. Pour cela on ne se restreindra pas aux tests conçus pas nous-même
mais on essayera aussi de trouver tous les cas de figure par la manière forte. Un générateur de code complètement
arbitraire est alors une bonne solution pour réaliser de tels tests. Dans notre cas, il permettra de mettre en
lumière défaut sur l'algorithme de worklist.
\\
Ce générateur de code est ajusté par un paramètre qu'est la quantité de variables affectées dans la déclaration. On
cherche également à ce que le code généré soit le plus proche possible de celui qu'aurait pu fournir un humain.
Dans notre syntaxe, il suffit pour le moment de donner une certaine répartition lors de la création des déclarations,
en donnant plus de chance à l'assignation d'apparaître, que des blocs Ifte ou Whiledo par exemple.
\subsection{Bugs}
\section{Conclusion}
\end{document}